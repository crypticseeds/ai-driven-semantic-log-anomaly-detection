# AI-Driven Semantic Log Anomaly Detection

A production-ready log analysis platform that combines semantic search, machine learning clustering, and LLM-powered reasoning to detect anomalies and provide root cause analysis in real-time.

## Tech Stack

### Core Technologies
- **HDBSCAN** - Hierarchical density-based clustering for semantic log grouping and outlier detection
- **Presidio** - Microsoft's PII detection and redaction framework for privacy compliance
- **Langfuse** - LLM observability and prompt management platform
- **Apache Kafka** - Distributed event streaming platform for log ingestion and processing
- **Qdrant** - Vector database for semantic embeddings storage and similarity search
- **OpenAI** - GPT models for embeddings generation and LLM-powered anomaly reasoning
- **Fluent Bit** - Lightweight log collector and forwarder for multi-source log ingestion
- **LangChain** - LLM orchestration framework for agent reasoning and tool calling
- **Doppler** - Secrets management platform for secure configuration and environment variable injection

### Infrastructure & Observability
- **FastAPI** - High-performance Python web framework
- **PostgreSQL** - Relational database for structured log storage
- **Next.js** - React-based dashboard frontend
- **OpenTelemetry** - Distributed tracing and instrumentation
- **Prometheus & Grafana** - Metrics collection and visualization
- **Tempo** - Distributed tracing backend

## Architecture Overview

### Log Processing Pipeline

The system processes logs through a multi-stage pipeline:

1. **Log Ingestion** - Fluent Bit collects logs from various sources and publishes to Kafka's `logs-raw` topic
2. **Stream Processing** - Kafka consumers process raw logs, extract metadata, and detect/redact PII using Presidio
3. **Semantic Embedding** - Processed logs are converted to vector embeddings using OpenAI's embedding models
4. **Storage** - Logs are stored in PostgreSQL (structured data) and Qdrant (vector embeddings)
5. **Anomaly Detection** - Hybrid detection pipeline combines statistical methods (IsolationForest, Z-score) with LLM validation
6. **Clustering** - HDBSCAN groups semantically similar logs and identifies outliers
7. **LLM Reasoning** - LangChain-powered agents provide root cause analysis and remediation guidance

### Kafka Integration

Apache Kafka serves as the central message broker for asynchronous log processing:

- **Topics:**
  - `logs-raw`: Raw log entries from Fluent Bit (3 partitions, 7-day retention)
  - `logs-processed`: Processed logs after PII redaction and normalization (3 partitions, 7-day retention)

- **Configuration:**
  - KRaft mode (no Zookeeper dependency)
  - Consumer group: `log-processor-group`
  - Automatic topic creation via initialization script

- **Kafbat UI:** Kafka UI integrated for topic monitoring, message browsing, and consumer group management.

The ingestion service consumes from `logs-raw`, processes each message through the pipeline (PII redaction, metadata extraction, embedding generation), and publishes processed logs to `logs-processed` for downstream consumers.

### Vector Database & Semantic Search

Qdrant stores log embeddings generated by OpenAI's text-embedding models, enabling:

- **Semantic Search**: Natural language queries to find similar log patterns
- **Similarity Matching**: Identify related logs based on semantic meaning rather than exact text matches
- **Cluster Analysis**: HDBSCAN operates on embeddings to group similar logs and detect outliers

### LLM-Powered Analysis

OpenAI GPT models provide:

- **Embedding Generation**: Convert log messages to high-dimensional vectors for semantic analysis
- **Anomaly Reasoning**: Validate statistical anomaly detections and provide explanations
- **Root Cause Analysis**: LangChain agents analyze anomalies with structured reasoning, identifying root causes with confidence scores
- **Remediation Guidance**: Actionable steps prioritized by severity

## Quick Start

### Prerequisites

- Docker and Docker Compose
- Doppler CLI (for secrets management)
- OpenAI API key (for embeddings and LLM features)

### Installation

1. **Install Doppler CLI:**

2. **Set up Doppler:**

   ```bash
   doppler login
   doppler projects create ai-log-analytics
   doppler setup --project ai-log-analytics --config dev
   ```

3. **Configure Secrets:**

   Required secrets in Doppler:
   - `DATABASE_URL` - PostgreSQL connection string
   - `KAFKA_BOOTSTRAP_SERVERS` - Kafka bootstrap servers (default: `localhost:9092`)
   - `OPENAI_API_KEY` - OpenAI API key
   - `QDRANT_URL` - Qdrant Cloud URL (optional)
   - `QDRANT_API_KEY` - Qdrant Cloud API key (optional)
   - `LANGFUSE_SECRET_KEY` - Langfuse secret key (optional)
   - `LANGFUSE_PUBLIC_KEY` - Langfuse public key (optional)
   - `LANGFUSE_HOST` - Langfuse host URL (optional)

4. **Start Services:**

   ```bash
   cd infra
   doppler run -- docker-compose up -d
   ```

### Service Endpoints


| Service         | Port | URL                                 |
| --------------- | ---- | ----------------------------------- |
| NextJs Frontend | 3000 | http://localhost:3000               |
| FastAPI Backend | 8000 | http://localhost:8000/docs          |
| Grafana         | 3000 | http://localhost:3001 (admin/admin) |
| Prometheus      | 9090 | http://localhost:9090               |
| Tempo           | 3200 | http://localhost:3200               |
| Kafka           | 9092 | localhost:9092 (binary protocol)    |
| Kafbat UI       | 8080 | http://localhost:8080               |
| PostgreSQL      | 5432 | localhost:5432                      |

## Key Features

### Hybrid Anomaly Detection

Two-tier detection pipeline combining fast statistical methods with LLM validation:

- **Tier 1**: IsolationForest, Z-score, and IQR methods for real-time detection
- **Tier 2**: LLM semantic validation for high-scoring anomalies to reduce false positives
- **Log-Level Awareness**: Weighted scoring based on log severity‚ÄîERROR/WARN logs are prioritized while INFO/DEBUG logs require significantly higher anomaly scores, reducing false positives from routine messages

### Semantic Clustering

HDBSCAN automatically groups logs by semantic similarity:

- Variable density clusters
- Automatic outlier detection (cluster ID -1)
- No pre-specified cluster count required
- Robust to noise

### PII Protection

Presidio automatically detects and redacts sensitive information:

- Email addresses, phone numbers, SSN, credit cards, IP addresses
- Original logs preserved for audit purposes
- Redaction applied at ingestion, search, and display stages

### LLM Reasoning Agent

LangChain-powered agent provides:

- Structured root cause analysis with confidence scores
- Prioritized remediation steps
- Severity assessment (LOW/MEDIUM/HIGH/CRITICAL)
- Cluster-aware analysis comparing outliers against normal patterns

### Budget Management

Built-in OpenAI API budget enforcement:

- Daily spending limits
- Automatic cost estimation
- Prometheus metrics for monitoring
- Cache-aware (cached embeddings don't count toward budget)

## API Endpoints

### Log Management
- `GET /api/v1/logs/search` - Semantic search with filtering
- `GET /api/v1/logs/{log_id}` - Get log by ID

### Clustering
- `POST /api/v1/logs/clustering/run` - Run HDBSCAN clustering
- `GET /api/v1/logs/clustering/clusters` - List clusters
- `GET /api/v1/logs/clustering/outliers` - Get outliers

### Anomaly Detection
- `POST /api/v1/logs/anomaly-detection/isolation-forest` - IsolationForest detection
- `POST /api/v1/logs/anomaly-detection/z-score` - Z-score detection
- `POST /api/v1/logs/anomaly-detection/iqr` - IQR detection
- `POST /api/v1/logs/anomaly-detection/score/{log_id}` - Score specific log

### Agent API (LangChain Tools)
- `POST /api/v1/agent/analyze-anomaly` - Analyze anomaly with root cause
- `POST /api/v1/agent/analyze-anomaly/{log_id}` - Analyze by log ID with cluster context
- `POST /api/v1/agent/detect-anomaly` - Detect if log is anomalous
- `POST /api/v1/agent/analyze-anomaly/stream` - Streaming analysis (SSE)
- `GET /api/v1/agent/tools` - List available agent tools

Full API documentation available at http://localhost:8000/docs

## Development

### Running Tests

```bash
python run_tests.py
```

### Project Structure

```
backend/
  app/
    api/v1/          # API endpoints
    services/        # Business logic services
    models/          # Data models
    db/              # Database models and sessions
    observability/   # OpenTelemetry, Prometheus, Langfuse

dashboard/
  nextjs-app/       # Next.js frontend

infra/
  docker/           # Dockerfiles
  docker-compose.yml
  kafka/            # Kafka topic initialization
  grafana/          # Grafana dashboards
  prometheus/       # Prometheus config
```

## Monitoring & Observability

- **Metrics**: Prometheus collects application and infrastructure metrics
- **Visualization**: Grafana dashboards for log volume, anomaly rates, and system health
- **Tracing**: OpenTelemetry distributed tracing with Tempo backend
- **LLM Observability**: Langfuse tracks LLM calls, costs, and prompt performance

## Contributing

Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request.

## License

See LICENSE file for details.

---

<div align="center">

### üîó Connect with Me

[![Portfolio](https://img.shields.io/badge/Portfolio-000?style=for-the-badge&logo=ko-fi&logoColor=white)](https://devopsfoundry.com/projects/)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/femi-akinlotan/)
[![Mail](https://img.shields.io/badge/Email-lightgrey?style=for-the-badge&logo=minutemailer&logoColor=white)](mailto:femi.akinlotan@devopsfoundry.com)

**Built with ‚ù§Ô∏è by Femi Akinlotan**

</div>
