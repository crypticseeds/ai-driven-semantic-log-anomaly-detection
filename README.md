# ai-driven-semantic-log-anomaly-detection
AI-driven semantic log anomaly detection project

## üöÄ Quick Start

### Prerequisites
- Docker and Docker Compose
- Doppler CLI (for secrets management)

## üîê Doppler Secrets Management

This project uses [Doppler](https://www.doppler.com/) for secure secrets management. All sensitive configuration values (API keys, database URLs, etc.) are stored in Doppler and injected at runtime.

### Installing Doppler CLI

**macOS:**
```bash
brew install dopplerhq/cli/doppler
```

**Linux (Debian/Ubuntu):**
```bash
sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl gnupg
curl -sLf --retry 3 --tlsv1.2 --proto "=https" 'https://packages.doppler.com/public/cli/gpg.DE2A7741A397C129.key' | sudo gpg --dearmor -o /usr/share/keyrings/doppler-archive-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/doppler-archive-keyring.gpg] https://packages.doppler.com/public/cli/deb/debian any-version main" | sudo tee /etc/apt/sources.list.d/doppler-cli.list
sudo apt-get update && sudo apt-get install doppler
```

**Other platforms:** See [Doppler CLI installation guide](https://docs.doppler.com/docs/install-cli)

### Setting Up Doppler

1. **Authenticate with Doppler:**
   ```bash
   doppler login
   ```

2. **Create a Doppler project:**
   ```bash
   doppler projects create ai-log-analytics
   ```

3. **Set up the project configuration:**
   ```bash
   doppler setup --project ai-log-analytics --config dev
   ```

4. **Add secrets to Doppler:**

   You can add secrets via the Doppler dashboard or CLI:

   ```bash
   # Add secrets via CLI
   doppler secrets set DATABASE_URL="postgresql://ailog:changeme@localhost:5432/ailog"
   doppler secrets set KAFKA_BOOTSTRAP_SERVERS="localhost:9092"
   doppler secrets set OPENAI_API_KEY="your-openai-api-key"
   doppler secrets set OPENAI_BUDGET="10.0"  # Daily budget in USD (optional)
   doppler secrets set QDRANT_URL="your-qdrant-url"
   doppler secrets set QDRANT_API_KEY="your-qdrant-api-key"
   doppler secrets set LANGFUSE_SECRET_KEY="your-langfuse-secret-key"
   doppler secrets set LANGFUSE_PUBLIC_KEY="your-langfuse-public-key"
   doppler secrets set LANGFUSE_HOST="http://langfuse:3000"
   ```

   **Required secrets:**
   - `DATABASE_URL` - PostgreSQL connection string
   - `KAFKA_BOOTSTRAP_SERVERS` - Kafka bootstrap servers
   - `OPENAI_API_KEY` - OpenAI API key for embeddings (optional)
   - `OPENAI_BUDGET` - Daily budget limit for OpenAI embeddings in USD (optional, default: unlimited)
   - `QDRANT_URL` - Qdrant Cloud URL (optional)
   - `QDRANT_API_KEY` - Qdrant Cloud API key (optional)
   - `LANGFUSE_SECRET_KEY` - Langfuse secret key (optional)
   - `LANGFUSE_PUBLIC_KEY` - Langfuse public key (optional)
   - `LANGFUSE_HOST` - Langfuse host URL (optional)

### Using Doppler with the Application

Run the application with Doppler CLI to automatically inject secrets as environment variables:

**For Local Development:**
```bash
# Run FastAPI app with Doppler
doppler run -- uvicorn app.main:app --host 0.0.0.0 --port 8000

# Or with Docker Compose
doppler run -- docker-compose up
```

**For CI/CD (with DOPPLER_TOKEN):**
```bash
# Set DOPPLER_TOKEN environment variable, then:
doppler run --token=$DOPPLER_TOKEN -- docker-compose up
```

The application automatically loads secrets from environment variables injected by the Doppler CLI. No SDK or special code is needed - the Doppler CLI injects secrets as environment variables, which your application reads normally.

### Configuration Priority

The application loads configuration in the following order:
1. **Environment variables** (from Doppler CLI injection or direct env vars)
2. **`.env` file** (if present)
3. **Default values** (for development only)

### Running the Stack

**With Doppler (Recommended):**
```bash
cd infra
doppler run -- docker-compose up -d
```

**Without Doppler (uses defaults/.env):**
```bash
cd infra
docker-compose up -d
```

## üìç Service Endpoints

Once the services are running, you can access them at the following endpoints:

### Services Overview

| Service | Port | Web UI? | URL / Access |
|---------|------|---------|--------------|
| **FastAPI Backend** | 8000 | ‚úÖ Yes | http://localhost:8000/docs |
| **Grafana** | 3001 | ‚úÖ Yes | http://localhost:3001 (admin/admin) |
| **Prometheus** | 9090 | ‚úÖ Yes | http://localhost:9090 |
| **Tempo** | 3200 | ‚ùå API only | `curl http://localhost:3200/ready` |
| **Kafka** | 9092 | ‚ùå Binary protocol | Use CLI (see below) |
| **PostgreSQL** | 5432 | ‚ùå Database protocol | Use psql (see below) |
| **Fluent Bit** | 2020 | ‚ùå Internal | Log forwarder (internal only) |

### API Endpoints

#### FastAPI Backend
- **Root**: `GET http://localhost:8000/`
- **Health Check**: `GET http://localhost:8000/health`
- **Health Check (Kafka)**: `GET http://localhost:8000/health/kafka`
- **API Docs**: `GET http://localhost:8000/docs`
- **ReDoc**: `GET http://localhost:8000/redoc`
- **Log Search**: `GET http://localhost:8000/api/v1/logs/search`
- **Get Log by ID**: `GET http://localhost:8000/api/v1/logs/{log_id}`
- **Run Clustering**: `POST http://localhost:8000/api/v1/logs/clustering/run`
- **List Clusters**: `GET http://localhost:8000/api/v1/logs/clustering/clusters`
- **Get Outliers**: `GET http://localhost:8000/api/v1/logs/clustering/outliers`
- **Detect Anomalies (IsolationForest)**: `POST http://localhost:8000/api/v1/logs/anomaly-detection/isolation-forest`
- **Detect Anomalies (Z-score)**: `POST http://localhost:8000/api/v1/logs/anomaly-detection/z-score`
- **Detect Anomalies (IQR)**: `POST http://localhost:8000/api/v1/logs/anomaly-detection/iqr`
- **Score Log Entry**: `POST http://localhost:8000/api/v1/logs/anomaly-detection/score/{log_id}`

### Programmatic Access

#### Kafka (Message Broker)
- **Bootstrap Server**: `localhost:9092`
- **Mode**: KRaft (no Zookeeper required)
- **Topics**: `logs-raw`, `logs-processed`

**Kafka KRaft Setup:**

This project uses Apache Kafka in KRaft (Kafka Raft) mode, which eliminates the need for Zookeeper. KRaft mode provides:
- Simplified deployment (no Zookeeper dependency)
- Faster startup times
- Better scalability
- Self-managed metadata

**Topics are automatically created** when the stack starts via the `kafka-init` service. The initialization script creates:
- `logs-raw`: Raw log entries from Fluent Bit (3 partitions, 1 week retention)
- `logs-processed`: Processed log entries after PII redaction and normalization (3 partitions, 1 week retention)

**Kafka CLI Commands:**
```bash
# List all topics
docker exec -it ai-log-kafka kafka-topics.sh --bootstrap-server localhost:9092 --list

# Describe a topic
docker exec -it ai-log-kafka kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic logs-raw

# Consume messages from logs-raw topic
docker exec -it ai-log-kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs-raw --from-beginning

# Consume messages from logs-processed topic
docker exec -it ai-log-kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs-processed --from-beginning

# Produce a test message
docker exec -it ai-log-kafka kafka-console-producer.sh --bootstrap-server localhost:9092 --topic logs-raw
```

**Kafka Health Check:**
```bash
# Check Kafka health via API
curl http://localhost:8000/health/kafka
```

**Kafka Configuration:**
- **Replication Factor**: 1 (single node for development)
- **Partitions**: 3 per topic (for parallel processing)
- **Retention**: 7 days or 1GB per topic
- **Consumer Group**: `log-processor-group`

#### OpenAI Budget Management

The system includes built-in budget enforcement for OpenAI API usage to control costs and prevent unexpected spending.

**Features:**
- **Daily Budget Limit**: Set a daily spending limit via `OPENAI_BUDGET` environment variable
- **Automatic Tracking**: Daily spending is automatically tracked and reset at midnight UTC
- **Budget Enforcement**: Requests are rejected when the daily budget is exceeded
- **Cost Estimation**: Estimates cost before making API calls to prevent budget overruns
- **Warning Thresholds**: Logs warnings when spending reaches 80% of budget
- **Prometheus Metrics**: Budget metrics exposed for monitoring (`openai_daily_spending_usd`, `openai_budget_exceeded_total`)
- **Cache-Aware**: Cached embeddings don't count toward the budget

**Configuration:**
```bash
# Set daily budget to $10 USD
doppler secrets set OPENAI_BUDGET="10.0"

# Or via environment variable
export OPENAI_BUDGET=10.0
```

**Budget Behavior:**
- Budget resets automatically at midnight UTC each day
- If budget is not set (or set to `None`), spending is unlimited
- When budget is exceeded, embedding requests return HTTP 429 (Too Many Requests)
- Budget tracking persists across service restarts (in-memory, resets on restart)
- For production, consider using Redis or a database for persistent budget tracking

**Monitoring:**
- View current daily spending: Prometheus metric `openai_daily_spending_usd`
- Track budget rejections: Prometheus counter `openai_budget_exceeded_total`
- Budget statistics available via `embedding_service.get_budget_stats()`

**Example:**
```python
# Get budget statistics
stats = embedding_service.get_budget_stats()
# Returns:
# {
#   "daily_budget_usd": 10.0,
#   "current_daily_spending_usd": 3.45,
#   "budget_remaining_usd": 6.55,
#   "budget_utilization_percent": 34.5,
#   "budget_enabled": True
# }
```

#### PII Detection and Redaction (Presidio)

This system uses [Microsoft Presidio](https://microsoft.github.io/presidio/) for automatic PII (Personally Identifiable Information) detection and redaction. All log entries are automatically scanned and redacted before storage and display.

**Features:**
- Automatic PII detection during log ingestion
- PII redaction in search results and API responses
- Support for multiple PII types: emails, phone numbers, SSN, credit cards, IP addresses, and more
- Original logs preserved in `raw_log` field for audit purposes

**PII Types Detected:**
- Email addresses ‚Üí `[EMAIL]`
- Phone numbers ‚Üí `[PHONE]`
- Credit card numbers ‚Üí `[CREDIT_CARD]`
- Social Security Numbers ‚Üí `[SSN]`
- IP addresses ‚Üí `[IP]`
- Person names, passport numbers, driver's licenses, and more

**Configuration:**
- PII service: `backend/app/services/pii_service.py`
- Full documentation: `docs/presidio-configuration.md`

**Usage:**
PII redaction happens automatically at three points:
1. **During Ingestion**: Logs are redacted before storing in PostgreSQL
2. **During Search**: Search results are redacted before being returned
3. **Dashboard Display**: Logs are redacted via API before display

#### HDBSCAN Semantic Clustering

This system uses [HDBSCAN](https://hdbscan.readthedocs.io/) (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for semantic clustering of log embeddings. HDBSCAN is a powerful clustering algorithm that groups similar log entries based on their semantic embeddings, enabling automatic pattern discovery and anomaly detection.

**What is HDBSCAN?**

HDBSCAN is an advanced clustering algorithm that extends DBSCAN with hierarchical clustering capabilities. It's particularly well-suited for:
- **Variable density clusters**: Handles clusters of different densities effectively
- **Outlier detection**: Automatically identifies outliers (noise points) as cluster ID -1
- **No pre-specified cluster count**: Automatically determines the optimal number of clusters
- **Robust to noise**: Handles noisy data better than traditional clustering methods

**How HDBSCAN Works in This Project:**

1. **Embedding Extraction**: Log entries are converted to semantic embeddings using OpenAI's embedding model and stored in Qdrant vector database
2. **Clustering**: HDBSCAN analyzes the embedding vectors to identify groups of semantically similar logs
3. **Cluster Assignment**: Each log entry is assigned to a cluster (or marked as an outlier with cluster_id = -1)
4. **Metadata Storage**: Cluster statistics, centroids, and representative logs are stored in the database
5. **Anomaly Detection**: Outliers (cluster_id = -1) are automatically flagged as anomalies

**Key Features:**

- **Semantic Clustering**: Groups logs based on meaning, not just exact text matches
- **Automatic Outlier Detection**: Identifies anomalous log entries that don't fit into any cluster
- **Variable Cluster Sizes**: Handles both large and small clusters effectively
- **No Manual Tuning**: Automatically determines optimal number of clusters
- **Persistent Storage**: Cluster assignments and metadata stored in PostgreSQL
- **Scalable**: Supports sampling for very large datasets

**Configuration:**

The clustering service can be configured via environment variables or settings:

```python
# Configuration options (with defaults)
HDBSCAN_MIN_CLUSTER_SIZE=5      # Minimum points to form a cluster
HDBSCAN_MIN_SAMPLES=3           # Minimum samples in neighborhood
HDBSCAN_SAMPLE_SIZE=None        # Optional: sample size for large datasets
HDBSCAN_CLUSTER_SELECTION_EPSILON=0.0  # Epsilon for cluster selection
HDBSCAN_MAX_CLUSTER_SIZE=None   # Optional: maximum cluster size
```

**Usage:**

The clustering service is available via the `ClusteringService` class:

```python
from app.services.clustering_service import clustering_service

# Perform clustering on all embeddings
result = clustering_service.perform_clustering()

# With custom parameters
result = clustering_service.perform_clustering(
    sample_size=10000,          # Sample 10k embeddings for large datasets
    min_cluster_size=10,         # Require at least 10 points per cluster
    min_samples=5                # Require at least 5 samples in neighborhood
)

# Result structure:
# {
#     "n_clusters": 15,           # Number of clusters found
#     "n_outliers": 42,           # Number of outliers (anomalies)
#     "cluster_assignments": {    # Mapping of log_id to cluster_id
#         "log-uuid-1": 0,
#         "log-uuid-2": 0,
#         "log-uuid-3": -1,       # -1 indicates outlier/anomaly
#         ...
#     },
#     "cluster_metadata": {       # Statistics for each cluster
#         0: {
#             "cluster_id": 0,
#             "cluster_size": 150,
#             "centroid": [...],  # Cluster center in embedding space
#             "representative_logs": ["uuid1", "uuid2", ...]
#         },
#         ...
#     }
# }

# Get information about a specific cluster
cluster_info = clustering_service.get_cluster_info(cluster_id=0)
```

**Cluster Results:**

- **Cluster IDs**: Positive integers (0, 1, 2, ...) represent valid clusters
- **Outlier ID**: `-1` represents outliers/anomalies that don't belong to any cluster
- **Cluster Metadata**: Each cluster stores its size, centroid, and representative log entries
- **Anomaly Detection**: Logs with `cluster_id = -1` are automatically marked as anomalies

**Benefits for Log Analysis:**

1. **Pattern Discovery**: Automatically groups similar log patterns together
2. **Anomaly Detection**: Identifies unusual log entries that don't match common patterns
3. **Root Cause Analysis**: Clusters help identify related issues and their scope
4. **Reduced Noise**: Focuses attention on clusters and outliers rather than individual logs
5. **Scalability**: Handles large volumes of logs efficiently with optional sampling

**Service Location:**

- Clustering service: `backend/app/services/clustering_service.py`
- Database models: `backend/app/db/postgres.py` (AnomalyResult, ClusteringMetadata)

#### Hybrid/Tiered Anomaly Detection Pipeline

This system uses a **hybrid two-tier detection approach** that combines fast statistical methods with intelligent LLM validation to achieve both speed and accuracy in anomaly detection.

**What is the Hybrid Approach?**

The hybrid pipeline uses two tiers:
- **Tier 1 (Fast Detection)**: Statistical methods (IsolationForest, Z-score, IQR) run on every log entry for real-time detection
- **Tier 2 (Smart Validation)**: LLM semantic validation runs only on high-scoring anomalies to reduce false positives and provide explanations

**How It Works:**

**Real-Time Pipeline (During Log Ingestion):**

1. **Tier 1 - Fast Statistical Detection**:
   - IsolationForest runs on every new log entry as it's stored
   - Provides immediate anomaly scoring
   - Fast and cost-effective (no LLM calls for normal logs)

2. **Tier 2 - LLM Validation** (conditional):
   - Triggers only when:
     - Tier 1 flags the log as anomalous (`is_anomaly = True`)
     - Anomaly score exceeds threshold (default: 0.7)
   - LLM validates the anomaly semantically
   - Provides explanation for why the log is anomalous
   - Reduces false positives by confirming true anomalies

3. **Result Storage**:
   - All results stored in `AnomalyResult` table
   - LLM reasoning stored in `llm_reasoning` field
   - Explanations always generated (fallback to explanation-only if detection fails)

**Batch Pipeline (HDBSCAN Clustering):**

1. **Tier 1 - HDBSCAN Clustering**:
   - Groups log embeddings into semantic clusters
   - Identifies outliers (cluster_id = -1) as potential anomalies

2. **Tier 2 - LLM Validation**:
   - Validates each outlier with LLM semantic analysis
   - Confirms or rejects HDBSCAN outlier classification
   - Generates explanations for confirmed anomalies

**Key Features:**

- **Cost-Effective**: LLM only runs on ~10-20% of logs (high-scoring anomalies)
- **Fast Real-Time**: Statistical methods provide immediate results
- **High Accuracy**: LLM validation reduces false positives
- **Always Explained**: Every anomaly gets LLM reasoning (detection + explanation or explanation-only fallback)
- **Configurable Thresholds**: Adjust when LLM validation triggers

**Configuration:**

The hybrid detection pipeline can be configured via environment variables:

```bash
# Anomaly Detection Thresholds
ANOMALY_SCORE_THRESHOLD=0.7              # Score threshold to trigger LLM validation (0.0-1.0)
LLM_VALIDATION_ENABLED=true              # Enable/disable LLM validation
LLM_VALIDATION_CONFIDENCE_THRESHOLD=0.6  # Minimum LLM confidence to confirm anomaly (0.0-1.0)
```

**Detection Methods Available:**

1. **IsolationForest** (default for real-time):
   - Unsupervised anomaly detection
   - Works well with high-dimensional embeddings
   - Fast and scalable

2. **Z-score**:
   - Statistical outlier detection
   - Based on standard deviations from mean
   - Good for normally distributed data

3. **IQR (Interquartile Range)**:
   - Statistical method using quartiles
   - Robust to outliers
   - Good for skewed distributions

4. **HDBSCAN** (batch):
   - Semantic clustering with automatic outlier detection
   - Groups similar logs, flags outliers

5. **LLM Validation** (Tier 2):
   - Semantic understanding of log content
   - Context-aware anomaly detection
   - Provides human-readable explanations

**Usage Example:**

```python
# Real-time detection happens automatically during log storage
# Results are stored in AnomalyResult table with:
# - detection_method: "IsolationForest" (or "HDBSCAN" for batch)
# - is_anomaly: True/False
# - anomaly_score: 0.0-1.0
# - llm_reasoning: Explanation text (if LLM validation ran)

# Query anomalies
from app.db.postgres import AnomalyResult
anomalies = db.query(AnomalyResult).filter(AnomalyResult.is_anomaly == True).all()

for anomaly in anomalies:
    print(f"Anomaly: {anomaly.anomaly_score:.2f}")
    print(f"Method: {anomaly.detection_method}")
    print(f"Explanation: {anomaly.llm_reasoning}")
```

**Benefits:**

1. **Speed**: Fast statistical methods provide immediate results
2. **Accuracy**: LLM validation reduces false positives
3. **Cost Control**: LLM only runs on high-confidence anomalies
4. **Explainability**: Every anomaly gets semantic explanation
5. **Flexibility**: Multiple detection methods available
6. **Scalability**: Handles high-volume log streams efficiently

**Service Locations:**

- Real-time detection: `backend/app/services/storage_service.py`
- Batch detection: `backend/app/services/clustering_service.py`
- Anomaly detection methods: `backend/app/services/anomaly_detection_service.py`
- LLM validation: `backend/app/services/llm_reasoning_service.py`
- Database models: `backend/app/db/postgres.py` (AnomalyResult)

#### PostgreSQL (Database)
- **Host**: `localhost`
- **Port**: `5432`
- **Database**: `ailog` (default)
- **User**: `ailog` (default)
- **Password**: `changeme` (default)
- **Connection String**: `postgresql://ailog:changeme@localhost:5432/ailog`

**PostgreSQL CLI:**
```bash
docker exec -it ai-log-postgres psql -U ailog -d ailog
```

#### Tempo (Distributed Tracing)
- **OTLP gRPC Endpoint**: `localhost:4317`
- **OTLP HTTP Endpoint**: `http://localhost:4318`
- **Tempo UI**: `http://localhost:3200`

### Container Management

**View running containers:**
```bash
docker ps
```

**View logs:**
```bash
docker-compose -f infra/docker-compose.yml logs -f [service-name]
```

**Stop all services:**
```bash
docker-compose -f infra/docker-compose.yml down
```

**Stop and remove volumes:**
```bash
docker-compose -f infra/docker-compose.yml down -v
```

## üìù Resume Bullet Points (You Can Copy)
AI Log Analyzer (FastAPI, LLMs, OpenTelemetry, Streamlit, Qdrant)
Designed and implemented an AI-powered log-analysis platform capable of ingesting live system logs, detecting anomalies, and generating LLM-driven root-cause analysis.
Built semantic search using vector embeddings (OpenAI + Qdrant) enabling natural-language querying across large log datasets.
Implemented anomaly detection models using PyOD (IsolationForest) with structured log parsing and clustering.
Developed a full observability stack (OpenTelemetry + Grafana + Prometheus) for tracing, metrics, and pipeline insights.
Delivered an interactive Streamlit dashboard visualizing log trends, anomaly timelines, and on-demand AI summaries.
